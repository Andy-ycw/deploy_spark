# Apache Spark Standalone Deployment

Why go through all the fuss to set up a cluster on the cloud to learn Spark? Although Spark APIs can be explored by running Spark locally, the important aspects of distributed computing can only be learned when coding in an environment close to production environments. Therefore, this project provides Terraform scripts and bash scripts to automate the Spark Standalone deployment on EC2 instances. This README describes the preparation before running the Terraform script for Spark Standalone deployment, how to run the Terraform script, and how to connect to the cluster in client mode.

An overview of the cluster setup is shown in the graph below. By default, three t2.micro instances would be spun up as a Bastion host, a Spark Master node, and a Spark worker node respectively. In a production environment, Spark is commonly set up as a private service that external users should not be able to interact with the cluster directly. To conform with this practice, the cluster is configured that only SSH connection is allowed from the Internet, and the communications within the cluster all happen within the subnet in which the EC2 instances are deployed. Upon the completion of deployment, users can connect to the Spark cluster from the local machine by SSH tunneling using the bastion host. Then, in the SSH session, users can run the PySpark interactive shell (submit a spark job in client mode under the hood), and start exploring Spark APIs.

If you encounter any problem when following along, please don't hesitate to raise it in Github Issues.

<p align="center">
  <img src="https://github.com/user-attachments/assets/e5dece3d-0ef1-43df-945a-69265da8f5ef" />
</p>

## Prerequesites
- Install Terraform following the [official website](https://developer.hashicorp.com/terraform/install).
- AWS user credentials have been configured locally. The user must be the default user in the `~/.aws/credentials` file. Also the user must have the `AmazonEC2FullAccess` and `IAMFullAccess` policies attached.
- Have an AWS role with the `AmazonS3FullAccess` policy attached.
- Have a SSH key pair.

The prerequisites above provide Terraform the necessary AWS privileges to perform the deployment tasks.

## Download software packages into a S3 bucket

- Download Hadoop 3.3.4 [here](https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/). Click on `hadoop-3.3.4.tar.gz`.
- Download Java 11 [here](https://www.openlogic.com/openjdk-downloads?field_java_parent_version_target_id=406&field_operating_system_target_id=426&field_architecture_target_id=391&field_java_package_target_id=All). Click on `.tar.gz` of the row `11.0.24+8`.
- Download Spark 3.5.1 [here](https://archive.apache.org/dist/spark/spark-3.5.1/). Click on `spark-3.5.1-bin-hadoop3.tgz`.

Upload all packages downloaded above to a S3 bucket. Later on the EC2 instances will be downloading the packages from the S3 bucket. This is consider a good practice because Spark is normally run in private networks. In AWS, downloading the packages from the Internet within private subnets requires a NAT gateway which incurs additional costs. On the other hand, the data transfer between EC2 instances and S3 within the same region is free. Although a VPC Gateway endpoint is required for the traffic between S3 and EC2 in private subnets, VPC Gateway enpoints do not incur costs as NAT gateways do.

## Deployment.
1. Clone this repo.
   ```bash
   git clone https://github.com/AndyIAO/deploy_spark.git
   PROJECT_ROOT=$(pwd)/deploy_spark
   cd $PROECT_ROOT
2. Create the `.confidential` directory to store local configuration. Move `load_config.sh` into the directory.
   ```bash
   mkdir .confidential
   mv load_config.sh .confidential/load_config.sh
3. Replace the first five environment variables in $PROJECT_ROOT/.confidential/load_config.sh with your configuration. The five variables looks like the below before you modify them.
   ```bash
   # Replace YOUR_KEY_PAIR_PATH with the path to your ssh public key.
   export TF_VAR_key_path=YOUR_KEY_PAIR_PATH

   # Replace YOUR_PKG_BUCKET_NAME with the name of the bucket to which the downloaded packages are uploaded.
   export TF_VAR_pkg_bucket=YOUR_PKG_BUCKET_NAME

   # Replace YOUR_IAM_ROLE with the name of the role with the `AmazonS3FullAccess` policy attached.
   export TF_VAR_iam_role=YOUR_IAM_ROLE

   # Replace YOUR_REGION with the region in which you want to deploy Spark (i.e. us-east-1).
   export TF_VAR_region=YOUR_REGION

   # Replace YOUR_SUBNET_CIDR with the CIDR block of the subnet in which you want to deploy Spark. (i.e. 172.41.41.0/20).
   export TF_VAR_subnet_cidr=YOUR_SUBNET_CIDR
4. Run run.sh to start the deployment. Then following the prompts generated by Terraform.
   ```bash
   cd $PROJECT_ROOT
   chmod +x run.sh
   ./run.sh
5. Run the command below to use the bastion host to perform SSH tunneling for the connection between port 8080 on your local machine and port 8080 on the Spark master node. SSH tunneling is necessary because the cluster is configured in a way that the only ingress traffic allowed from the Internet is SSH.
   ```bash
   # Replace MASTER_NODE_PRIVATE_IP and BASTION_NODE_PUBLIC_IP with the IPs available in the AWS Console.
   ssh -L 8080:MASTER_NODE_PRIVATE_IP:8080 -i SSH_PRIVATE_KEY_PATH ec2-user@BASTION_NODE_PUBLIC_IP

   # i.e. ssh -L 8080:172.72.72.72:8080 -i ~/.ssh/your_key.pem ec2-user@52.52.52.52
6. Wait for a couple minutes and enter `localhost:8080` in your local browser. If you are able to lauch Spark UI, it means the Spark Master is up and running. If fail to launch the Spark UI, it means the installation has not finished in the master node. Refresh after a couple minutes. Normally, the Spark UI should be available within 5 minutes after the Terraform script is run.
7. Check the `Alive Workers` in the Spark UI. With the default configuration in `$PROJECT_HOME/load_config.sh`, the number of alive workers should be 1, and this means the deployment of Spark Standalone has been completed! If the number is 0, refresh the page every other 10 seconds until the number becomes 1.
   - In `$PROJECT_ROOT/.confidential/load_config.sh`, `TF_VAR_spark_executor_num` can be modified if you want to run more than one worker instance. Also, `TF_VAR_ec2_slave` can be assigned with more powerful instance types (default is t2.micro). If the instance type is changed from the default, `TF_VAR_spark_executor_memory` and `TF_VAR_spark_executor_core` should be changed according to the computing resources of the newly assigned instance type.
      - In fact, the default `t2.micro` is not suitable for running Spark because the memory is too small. The default is set so because `t2.micro` is avaible in free tier. To follow the tutorial in Spark documentation, it is recommended to assign `TF_VAR_ec2_slave` with `t2.small` and `TF_VAR_spark_executor_memory` with `1g`, which would incur some costs because the instance is not available in free tier.
8. To start a PySpark interactive session, which submit a Spark job in client mode, run the commands below in the SSH tunneling terminal from step 5. Then, you should be able to explore Spark interactively!
   ```bash
   source /home/.bashrc
   cd $SPARK_HOME
   $SPARK_HOME/bin/pyspark
9. Finally, remember to terminate the EC2 instances in AWS consoles. You can use Terraform to terminate instances too. I will leave to you to explore how do that.
